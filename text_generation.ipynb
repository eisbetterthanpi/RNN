{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/RNN/blob/main/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title gen text eg\n",
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A1Y1pqGImvBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yG_n40gFzf9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "6cf9578f-269c-43e5-8005-8a65ab8fe503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n",
            "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# @title tf data\n",
        "# https://www.tensorflow.org/text/tutorials/text_generation\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "vocab = sorted(set(text))\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "# use the get_vocabulary() method of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way.\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "print(all_ids)\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "# for ids in ids_dataset.take(10):\n",
        "#     print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "seq_length = 100\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # convert individual chars to seqs of the desired len\n",
        "\n",
        "# Each input sequence will contain seq_length characters from the text.\n",
        "# For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "# So break the text into chunks of seq_length+1\n",
        "def split_input_target(sequence):\n",
        "    return sequence[:-1], sequence[1:] # input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "# for input_example, target_example in dataset.take(1):\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, so it doesn't attempt to shuffle the entire sequence in memory.\n",
        "# Instead,it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title tf model\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "        if return_state: return x, states\n",
        "        else: return x\n",
        "\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "model = MyModel(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "outputId": "8c6b9413-796d-4b93-a635-8e4a48fe30fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# sample from the output distribution, to get actual character indices\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "print(sampled_indices)\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "\n",
        "# check that the exponential of the mean loss is approximately equal to the vocabulary size\n",
        "tf.exp(example_batch_mean_loss).numpy()\n",
        "\n",
        "# Configure the training procedure\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title tf generate\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids), # Put a -inf at each bad index.\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1) # Sample output logits\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "    return predicted_chars, states\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title inference\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "# next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "start = time.time()\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "end = time.time()\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))\n",
        "print('\\nRun time:', end - start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eIBHmKTxyws9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0pZ101hjwW0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title CustomTraining\n",
        "# The above training procedure is simple, but does not give you much control. It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes\n",
        "# https://www.tensorflow.org/guide/eager\n",
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables) # tf.GradientTape to track the gradients\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return {'loss': loss}\n",
        "\n",
        "model = CustomTraining(vocab_size=len(ids_from_chars.get_vocabulary()), embedding_dim=embedding_dim, rnn_units=rnn_units)\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "model.fit(dataset, epochs=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tSNwymzf-q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title custom train\n",
        "\n",
        "# complete custom training loop\n",
        "EPOCHS = 10\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title data\n",
        "# https://github.com/Sam-Armstrong/tinyGPT/blob/main/Training.py\n",
        "# https://colab.research.google.com/github/karpathy/minGPT/blob/master/play_char.ipynb\n",
        "# https://github.com/karpathy/nanoGPT\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        data = ''.join(raw_data)\n",
        "        chars = sorted(list(set(data)))\n",
        "        self.vocab_size = len(chars) # 283\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data) - self.seq_len\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # dix = self.data[idx:idx + self.seq_len + 1]\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "import requests\n",
        "url='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "\n",
        "out=requests.get(url)\n",
        "with open(\"shakespeare.txt\", \"wb\") as f:\n",
        "    f.write(out.content)\n",
        "text=out.content.decode(encoding='utf-8')\n",
        "\n",
        "# vocab = sorted(set(text))\n",
        "\n",
        "# data = list(open('input.txt', 'r').read()) # for using a text corpus contained within a .txt file\n",
        "# from torchtext.datasets import WikiText2\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "seq_len = 128\n",
        "train_dataset = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "# test_dataset = CharDataset(test_iter, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 512 #512\n",
        "train_loader = DataLoader(train_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# test_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "def encode(context): return torch.tensor([train_dataset.stoi.get(c) for c in context], device=device)\n",
        "def decode(x): return ''.join([train_dataset.itos[int(i)] for i in x])\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "# n=2\n",
        "# print(decode(x[n]))\n",
        "# print(decode(y[n]))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kuGXrz4F3uOT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title torch model\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # self.gru = nn.GRU(embedding_dim, rnn_units, batch_first=True, return_sequences=True)\n",
        "        self.gru = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = torch.zeros(1, x.size(0), rnn_units, device=device)\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        if hidden is not None:  # Maintain hidden state for potential sequential usage\n",
        "            return output, hidden\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "vocab_size = train_dataset.vocab_size\n",
        "# vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "model = MyModel(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units).to(device)\n",
        "# model.summary()\n"
      ],
      "metadata": {
        "id": "cqXZmnwlYASO",
        "cellView": "form"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ],
      "metadata": {
        "id": "zOB1Kh3jL6YV",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "import time\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3, (0.9, 0.999), eps=1e-08)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    start = time.time()\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optimizer)\n",
        "    # test(test_loader, model, loss_fn)\n",
        "    print(time.time()-start)\n",
        "    print(generate(model, \"A \"))\n",
        "print(\"Done!\")\n",
        "torch.save(model.state_dict(), \"model.pth\") # save model weights to 'model.pth'\n",
        "# model = NeuralNetwork(28*28, 10) # create new model\n",
        "# model.load_state_dict(torch.load(\"model.pth\")) # load model weights from 'model.pth'\n",
        "\n"
      ],
      "metadata": {
        "id": "UFGUIC4tiT9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1cc80f8-2c0d-4a7a-e6e0-a80a8f5a875d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 4.184251  [    0/ 8646]\n",
            "8.548993349075317\n",
            "A pist.\n",
            "\n",
            "\n",
            "OEN-Fcb?z\n",
            "WRD;Bhin ne.\n",
            "B Inen'that I ent cour; athave, h\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.494669  [    0/ 8646]\n",
            "8.65460467338562\n",
            "A Rad is thowon yt g'sengadt brewse,\n",
            "Se prenercet\n",
            "in kortun,\n",
            "We, a\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.215699  [    0/ 8646]\n",
            "8.724737882614136\n",
            "A GFud pide.\n",
            "Bung'd tould thenteis forr.\n",
            "\n",
            "LUORDENRTD:\n",
            "Yole'tion:ot\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.047531  [    0/ 8646]\n",
            "8.689782857894897\n",
            "A Endiked:\n",
            "Undis, and it.\n",
            "Whis huse ow of cousion aperion th-\n",
            "Vir,\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.904373  [    0/ 8646]\n",
            "8.576466798782349\n",
            "A nonou your all surde his evey be with by frieds, my did it years\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.801122  [    0/ 8646]\n",
            "8.503104209899902\n",
            "A Lender:\n",
            "\n",
            "Firsill Cipevins, I cell musinage man and his wive\n",
            "Byce\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.716901  [    0/ 8646]\n",
            "8.546370029449463\n",
            "A faist with her word?\n",
            "\n",
            "ROLICE:\n",
            "Or Camen his iffie.\n",
            "\n",
            "VOLIANUS:\n",
            "Goo\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.653911  [    0/ 8646]\n",
            "8.555143594741821\n",
            "A I known,\n",
            "These prothen forth ofcace to heaven! the genel.\n",
            "\n",
            "JULIN\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.568285  [    0/ 8646]\n",
            "8.559444427490234\n",
            "A justich being revoines\n",
            "Merclut that well or breath,\n",
            "If I swords \n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.529017  [    0/ 8646]\n",
            "8.566006183624268\n",
            "A news the cuses and gent to stower:\n",
            "When is noble disoners to; fa\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.486853  [    0/ 8646]\n",
            "8.586951494216919\n",
            "A a wind!\n",
            "You and spiceing did what as leave all answer.\n",
            "Pray the \n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.454771  [    0/ 8646]\n",
            "8.61863112449646\n",
            "A was no presurp,\n",
            "And conceived me, in that hall humblehing: but t\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.425901  [    0/ 8646]\n",
            "8.592644453048706\n",
            "A murder, be thank at those of joy in greater it savisy deying.\n",
            "\n",
            "K\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.390949  [    0/ 8646]\n",
            "8.564356088638306\n",
            "A not play to have him to envy.\n",
            "\n",
            "VOLUMINA:\n",
            "Why, how no mother?\n",
            "\n",
            "HE\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.372208  [    0/ 8646]\n",
            "8.61428451538086\n",
            "A bear as hows:\n",
            "Now if yet unpossessar bloody eyes in his part;\n",
            "He\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.342519  [    0/ 8646]\n",
            "8.557250499725342\n",
            "A provided\n",
            "A seek grave of all this orthing inferried\n",
            "Put of croun\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.323302  [    0/ 8646]\n",
            "8.570591926574707\n",
            "A whole heap?\n",
            "\n",
            "SICINIUS:\n",
            "Thou beseirf! what though alas, come to d\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.302049  [    0/ 8646]\n",
            "8.51158094406128\n",
            "A Darchard's from Worthumous,\n",
            "This drunth I did hearts? Come you a\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.304934  [    0/ 8646]\n",
            "8.542149305343628\n",
            "A thank your presence; and when mine wouldst kind as Cominius,\n",
            "Fou\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.269287  [    0/ 8646]\n",
            "8.581872701644897\n",
            "A now for whether the greater mine; Henry mished\n",
            "To sufe to hear t\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(generate(model, \"A \",500))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBBwHlaTtSI3",
        "outputId": "750701a2-68b5-4dcf-c367-1a29dcf1e023"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A here most sleep\n",
            "The strangers upon myself kiss our guess\n",
            "Dight upon small meeps their counterately.\n",
            "\n",
            "BENVOLIO:\n",
            "Papelous! Why, as it brought is tender prepared,\n",
            "Intone themselves that must ne'er whils yare?\n",
            "What have I may have none, ye not how it.\n",
            "\n",
            "PAULINA:\n",
            "How! courdering there; indeed, I comes?\n",
            "Why not hear the most vaults are blood!\n",
            "\n",
            "GLOUCIO:\n",
            "Sirrah, the Prettians are sovereign.\n",
            "God find good, my sovereign'st give me keeps about!\n",
            "The huttiffer broaches, and given us be as stone\n",
            "And orinables \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}